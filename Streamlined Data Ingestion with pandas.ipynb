{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzt6MdkxiAY2Ml0gVvXUNH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirmalaraj77/Python/blob/main/Streamlined%20Data%20Ingestion%20with%20pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Streamlined Data Ingestion with pandas**\n"
      ],
      "metadata": {
        "id": "AP6li-VVaAZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Notes**\n",
        "\n",
        "### **Plot the total number of tax returns by income group**\n",
        "* counts = data.groupby(\"agi_stub\").N1.sum()\n",
        "* counts.plot.bar()\n",
        "* plt.show()\n",
        "\n",
        "### **View counts of dependents and tax returns by income level**\n",
        "* print(data.groupby(\"agi_stub\").sum())\n",
        "\n",
        "### **check if 2 dfs are equal**\n",
        "* tax_data_v1.equals(tax_data_v2)\n",
        "\n",
        "###**find data with NA values**\n",
        "* print(tax_data[tax_data.zipcode.isna()])\n",
        "\n",
        "### **View Unique years in data**\n",
        "print(all_responses.Year.unique())\n",
        "\n"
      ],
      "metadata": {
        "id": "wnO2HUkJdeIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Flat Files**\n",
        "* Load pandas\n",
        "* Import files\n",
        "* Use sep = '\\t' for TSV\n"
      ],
      "metadata": {
        "id": "JZHQ_r8KapMt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rOJUkLBbZcs5"
      },
      "outputs": [],
      "source": [
        "#  load pandas and df\n",
        "import pandas as pd\n",
        "\n",
        "vermont_url = 'https://assets.datacamp.com/production/repositories/4412/datasets/61bb27bf939aac4344d4f446ce6da1d1bf534174/vt_tax_data_2016.csv'\n",
        "free_code_camp_url = 'https://assets.datacamp.com/production/repositories/4412/datasets/fdb113aa942a0e0ad5c155b2f04784886f0038c9/fcc-new-coder-survey.xlsx'\n",
        "nyc_weather_url = 'https://github.com/nirmalaraj77/datasets/blob/main/data.db'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modify flat file imports**\n",
        "\n",
        "### **limit columns - usecols**\n",
        "* accepts list of col numbers, names or function\n",
        "\n",
        "### **limit rows - nrows and skiprows**\n",
        "* accepts list of row numbers, number of rows or function\n",
        "* set header =None so pandas knows there are no column names\n",
        "* get rows 1000 to 1500\n",
        "* df = pd.read_csv ('xxx.csv', nrows = 1000, skiprows = 1000, header=None)\n",
        "\n",
        "### **assign column names**\n",
        "* names argument\n",
        "* list MUST include name for every column\n",
        "* can get list from previous df to reuse -> col_names = list(df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jkpwYlHgelD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load vermont db\n",
        "import pandas as pd\n",
        "vermont = pd.read_csv(vermont_url)\n",
        "\n",
        "# find shape\n",
        "print(vermont.shape)\n",
        "\n",
        "# get col names\n",
        "col_names = list(vermont)\n",
        "print(col_names)\n",
        "\n",
        "# load 1000 - 1250 rows with column names\n",
        "vermont_1000_1250 = pd.read_csv(vermont_url, nrows = 251, skiprows = 1000, header=None, names = col_names)\n",
        "print(vermont_1000_1250.head())\n",
        "\n",
        "# print dtypes\n",
        "print(vermont_1000_1250.dtypes)\n"
      ],
      "metadata": {
        "id": "gvJPMnbmmE3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Error Handling**\n",
        "\n",
        "###**Specify data types**\n",
        "\n",
        "* dtype - takes dictionary of column name and date type\n",
        "* non-standard dtypes like pandas categories must be passed in quotations\n",
        "\n",
        "###**Customize missing data values**\n",
        "\n",
        "* na_values - takes single value, list or dict of colmns and values\n",
        "\n",
        "###**Lines with errors**\n",
        "\n",
        "* try - except\n",
        "* error_bad_lines=False - skip unparsable records\n",
        "* warn_bad_lines=True - see messages when records are skipped\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NWxwOP6Uo1Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find vermont dtypes\n",
        "print(vermont.dtypes)\n",
        "\n",
        "# set dict to change zipcode and agi_stub columns dtypes\n",
        "data_types = {'agi_stub':'category',\n",
        "\t\t\t  'zipcode' : 'str'}\n",
        "\n",
        "# load vermont csv specifying types\n",
        "vermont = pd.read_csv(vermont_url, dtype = data_types)\n",
        "\n",
        "# find vermont dtypes again\n",
        "print(vermont.dtypes.head())"
      ],
      "metadata": {
        "id": "0Dtsh42i206K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dict specifying that 0s in zipcode are NA values\n",
        "null_values = {'zipcode': 0}\n",
        "\n",
        "# Load csv using na_values keyword argument\n",
        "vermont = pd.read_csv(vermont_url,\n",
        "                   na_values = null_values)\n",
        "\n",
        "# View rows with NA ZIP codes\n",
        "print(vermont[vermont.zipcode.isna()])"
      ],
      "metadata": {
        "id": "GgT2OD8t5ner"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # Set warn_bad_lines to issue warnings about bad records\n",
        "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\",\n",
        "                     error_bad_lines=False,\n",
        "                     warn_bad_lines=True)\n",
        "\n",
        "  # View first 5 records\n",
        "  print(data.head())\n",
        "\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Your data contained rows that could not be parsed.\")"
      ],
      "metadata": {
        "id": "oxDxf5yO93n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Excel Files**\n",
        "\n",
        "* pd.read_excel\n",
        "* nrows\n",
        "* skiprows\n",
        "* usecols - name, positional number,function or letter (e.g. A:P)\n",
        "\n",
        "\n",
        "###**Multiple Sheets**\n",
        "\n",
        "* sheet_name - string, 0 indexed position numbers, list\n",
        "* any other arguments apply to all sheets\n",
        "\n",
        "\n",
        "###**Load all sheets**\n",
        "\n",
        "* sheet_name=None - outputs dictionary\n",
        "* keys - sheet names\n",
        "* values  - dataframes for each sheet\n",
        "* create empty df and itearte through dict\n",
        "* add columns for keys\n",
        "* add dict to concat function\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1cio6BVDxXc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "col_string = 'A:H'\n",
        "data_types = {'A': 'int64'}\n",
        "\n",
        "survey_data = pd.read_excel(free_code_camp_url,\n",
        "                            skiprows=2,\n",
        "                            usecols=col_string)\n",
        "\n",
        "survey_data.head(1)"
      ],
      "metadata": {
        "id": "g2ZuZv8Mxd0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load sheet 1 with index no.\n",
        "survey_data_sheet1 = pd.read_excel(free_code_camp_url,\n",
        "                                   sheet_name=1)\n",
        "\n",
        "# load sheet 2 with name\n",
        "survey_data_sheet2 = pd.read_excel(free_code_camp_url,\n",
        "                                   sheet_name='2017')\n",
        "\n",
        "# check if they are equal\n",
        "print(survey_data_sheet1.equals(survey_data_sheet2))"
      ],
      "metadata": {
        "id": "QghG7KbXDfAY",
        "outputId": "2a356c49-8d52-4105-e4c5-2211bc6bd4cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load all sheets\n",
        "survey_data_all = pd.read_excel(free_code_camp_url,\n",
        "                               sheet_name=None)\n",
        "\n",
        "# check type\n",
        "print(type(survey_data_all))\n",
        "\n",
        "# iterate over dict\n",
        "for key, value in survey_data_all.items():\n",
        "  print(key, type(value))\n",
        "\n",
        "# create empty df\n",
        "all_responses = pd.DataFrame()\n",
        "\n",
        "# iterate through dictionary\n",
        "for sheet_name , frame in survey_data_all.items():\n",
        "  # add column so we know which year data id from\n",
        "  frame[\"Year\"] = sheet_name\n",
        "\n",
        "  # add the dataframe to all responses\n",
        "  all_responses = pd.concat([all_responses, frame])\n",
        "\n",
        "# View years in data\n",
        "print(all_responses.Year.unique())\n",
        "\n"
      ],
      "metadata": {
        "id": "696x3b_pzfxy",
        "outputId": "dc24a694-8b18-491e-d665-56fa0af060ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "2016 <class 'pandas.core.frame.DataFrame'>\n",
            "2017 <class 'pandas.core.frame.DataFrame'>\n",
            "['2016' '2017']\n"
          ]
        }
      ]
    }
  ]
}