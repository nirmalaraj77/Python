{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMV9NNmJnBvqTpuzSJuV3b+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirmalaraj77/Python/blob/main/Cleaning_Data_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning Data in Python**\n",
        "\n",
        "* .info()\n",
        "* .describe()\n",
        "* assert\n",
        "* new unnamed index column - index_col = 'Unnamed: 0'\n",
        "* ~ subset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L4EZ2dCUmFjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import practise datasets\n",
        "import pandas as pd\n",
        "\n",
        "airlines = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/airlines_final.csv', index_col = 'Unnamed: 0')\n",
        "banking = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/banking_dirty.csv', index_col = 'Unnamed: 0')\n",
        "ride_sharing = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/ride_sharing_new.csv', index_col = 'Unnamed: 0')\n",
        "restaurants = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/restaurants_L2.csv', index_col = 'Unnamed: 0')\n",
        "restaurants_new = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/restaurants_L2_dirty.csv', index_col = 'Unnamed: 0')"
      ],
      "metadata": {
        "id": "oLyqa7dimYCS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **String to Integers**\n",
        "\n",
        "###Remove '£' from Revenue column\n",
        "* Sales['Revenue'] = Sales['Revenue'].str.strip('£')\n",
        "\n",
        "###Convert Revenue column to integer\n",
        "* Sales['Revenue'] = Sales['Revenue'].astype('int')\n",
        "\n",
        "###Verify datatype\n",
        "* assert Sales['Revenue'].dtype == 'int'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g0AB6xEZucsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Numeric to Categorical**\n",
        "\n",
        "###Convert to category\n",
        "* df['col_name'] = df['col_name'].astype('category')\n",
        "\n"
      ],
      "metadata": {
        "id": "h0sDzIT3wNuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())\n",
        "\n",
        "# Convert user_type from integer to category\n",
        "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
        "\n",
        "# Write an assert statement confirming the change\n",
        "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
        "\n",
        "# Print new summary statistics\n",
        "print(ride_sharing['user_type_cat'].describe())"
      ],
      "metadata": {
        "id": "hOzzBodsy_47",
        "outputId": "8db9b69d-206d-4431-ccc5-7baebdfc8a71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 25760 entries, 0 to 25759\n",
            "Data columns (total 11 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   duration         25760 non-null  object\n",
            " 1   station_A_id     25760 non-null  int64 \n",
            " 2   station_A_name   25760 non-null  object\n",
            " 3   station_B_id     25760 non-null  int64 \n",
            " 4   station_B_name   25760 non-null  object\n",
            " 5   bike_id          25760 non-null  int64 \n",
            " 6   user_type        25760 non-null  int64 \n",
            " 7   user_birth_year  25760 non-null  int64 \n",
            " 8   user_gender      25760 non-null  object\n",
            " 9   duration_trim    25760 non-null  object\n",
            " 10  duration_time    25760 non-null  int64 \n",
            "dtypes: int64(6), object(5)\n",
            "memory usage: 2.4+ MB\n",
            "None\n",
            "count    25760.000000\n",
            "mean         2.008385\n",
            "std          0.704541\n",
            "min          1.000000\n",
            "25%          2.000000\n",
            "50%          2.000000\n",
            "75%          3.000000\n",
            "max          3.000000\n",
            "Name: user_type, dtype: float64\n",
            "count     25760\n",
            "unique        3\n",
            "top           2\n",
            "freq      12972\n",
            "Name: user_type_cat, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip duration of minutes\n",
        "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
        "\n",
        "# Convert duration to integer\n",
        "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
        "\n",
        "# Write an assert statement making sure of conversion\n",
        "assert ride_sharing['duration_time'].dtype == 'int'\n",
        "\n",
        "# Print formed columns and calculate average ride duration\n",
        "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
        "print(ride_sharing['duration_time'].mean())"
      ],
      "metadata": {
        "id": "Za_LsjdUzCPk",
        "outputId": "ea5d87ac-7c13-4e70-a732-11f9f1190efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         duration duration_trim  duration_time\n",
            "0      12 minutes           12              12\n",
            "1      24 minutes           24              24\n",
            "2       8 minutes            8               8\n",
            "3       4 minutes            4               4\n",
            "4      11 minutes           11              11\n",
            "...           ...           ...            ...\n",
            "25755  11 minutes           11              11\n",
            "25756  10 minutes           10              10\n",
            "25757  14 minutes           14              14\n",
            "25758  14 minutes           14              14\n",
            "25759  29 minutes           29              29\n",
            "\n",
            "[25760 rows x 3 columns]\n",
            "11.389052795031056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Out of range dates**\n",
        "\n",
        "* import datetime as dt\n",
        "* today_date = dt.date.today()\n",
        "* find dates in the future\n",
        "* df['subs_date'] > today_date"
      ],
      "metadata": {
        "id": "Z1PAwk2_2LrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "today_date = dt.date.today()\n",
        "today_date\n",
        "birthday = dt.date(1999, 10, 2)\n",
        "today_date < birthday"
      ],
      "metadata": {
        "id": "538ozAVo2wn6",
        "outputId": "c4938121-7972-4798-c061-20e71762ac4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deal with out of range data**\n",
        "\n",
        "* drop\n",
        "* setting custom minimum and maximums\n",
        "* treat as missing and impute\n",
        "* setting custom value depending on business assumption\n",
        "\n",
        "### Drop values using filtering\n",
        "* movies = movies['rating'] <= 5\n",
        "\n",
        "### Drop values using .drop\n",
        "movies.drop(movies[movies['rating'] > 5].index, inplace = True)\n",
        "\n",
        "### Set custom maximum value\n",
        "* movies.loc[movies['rating'] > 5, 'rating'] = 5\n",
        "\n",
        "### Convert to date\n",
        "* import datetime as dt\n",
        "* movie['signup'] = pd.to_datetime(movie['signup']).dt.date\n",
        "\n",
        "\n",
        "### Drop future dates using filtering or .drop\n",
        "\n",
        "### Hardcode dates with upper limit\n",
        "today_date = dt.date.today()\n",
        "* movies.loc[movies['signup'] > today_date, 'signup'] = today_date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QE_lwxyn4l8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uniqeness Constraints**\n",
        "\n",
        "* **Subsetting on metadata and keeping all duplicate records gives you a better bird-eye's view over your data and how to duplicate it**\n",
        "\n",
        "### Find Duplicates\n",
        "\n",
        "* .duplicated()\n",
        "* True for duplicated and False for non-duplicated\n",
        "* subset: list of column names to check for duplication\n",
        "* keep: whether to keep 'first', 'last' or all ('False') duplicate values\n",
        "* .sort_values(by = 'col_name')\n",
        "\n",
        "### Drop full duplicates\n",
        "\n",
        "* .drop_duplicates()\n",
        "* subset: list of column names to check for duplication\n",
        "* keep: whether to keep 'first', 'last' or all ('False') duplicate values\n",
        "* inplace: Drop duplicated directly in df ('True')\n",
        "\n",
        "### Drop partial duplicates\n",
        "\n",
        "* Group by column names and produce statistical summaries\n",
        "* .goupby() and .agg()\n",
        "* .reset_index() - numbered indices in final output\n",
        "* column_names = ['first_name', 'last_name', 'addres']\n",
        "* summaries = {'height' : 'max', 'weight' : 'mean'}\n",
        "* height_weight = height_weight.groupby(by = column_names).agg(summaries).reset_index()\n",
        "*\n",
        "\n",
        "### E.g.\n",
        "* Find duplicates\n",
        "* duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
        "\n",
        "* Sort your duplicated rides\n",
        "* duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
        "\n",
        "* Print relevant columns\n",
        "* print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
        "\n",
        "### E.g.\n",
        "* Drop complete duplicates from ride_sharing\n",
        "* ride_dup = ride_sharing.drop_duplicates()\n",
        "\n",
        "* Create statistics dictionary for aggregation function\n",
        "* statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
        "\n",
        "* Group by ride_id and compute new statistics\n",
        "* ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
        "\n",
        "* Find duplicated values again\n",
        "* duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
        "* duplicated_rides = ride_unique[duplicates == True]\n",
        "\n",
        "* Assert duplicates are processed\n",
        "* assert duplicated_rides.shape[0] == 0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ulvBBORO4_VA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categories and Membership Constraints**\n",
        "\n",
        "### Find inconsistent categories\n",
        "\n",
        "* .set and .difference\n",
        "* create list of categories\n",
        "* find categories in df not in list\n",
        "* find rows in df matching inconsistent categories\n",
        "* .isin\n",
        "* subset df with boolean result\n",
        "\n",
        "###  Drop inconsistent categories and get consistent categories only\n",
        "\n",
        "* ~ subset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FoB6Vf_x4n03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use airlines dataset\n",
        "airlines.head()\n",
        "\n",
        "# create dictionary of categories\n",
        "cat_all = {'cleanliness' : ['Clean', 'Average', 'Somewhat clean', 'Somewhat dirty', 'Dirty'],\n",
        "           'safety' : ['Neutral', 'Very safe', 'Somewhat safe', 'Very unsafe', 'Somewhat unsafe'],\n",
        "           'satisfaction' : ['Very satisfied', 'Neutral', 'Somewhat satisfied', 'Somewhat unsatisfied', 'Very unsatisfied']}\n",
        "\n",
        "# create categories df\n",
        "categories = pd.DataFrame (cat_all)\n",
        "\n",
        "# Print unique values of survey columns in airlines\n",
        "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
        "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
        "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")\n",
        "\n",
        "# Find the cleanliness category in airlines not in categories\n",
        "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
        "\n",
        "# Find rows with that category\n",
        "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
        "\n",
        "# Print rows with inconsistent category\n",
        "print(airlines[cat_clean_rows])\n",
        "\n",
        "# Print rows with consistent categories only\n",
        "# print(airlines[~cat_clean_rows])"
      ],
      "metadata": {
        "id": "coRjPLPH63X0",
        "outputId": "18d4e8c8-5d4d-4a6e-c1a1-468b9b936db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleanliness:  ['Clean' 'Average' 'Unacceptable' 'Somewhat clean' 'Somewhat dirty'\n",
            " 'Dirty'] \n",
            "\n",
            "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
            "\n",
            "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
            " 'Very unsatisfied'] \n",
            "\n",
            "       id        day           airline  destination  dest_region dest_size  \\\n",
            "4    2992  Wednesday          AMERICAN        MIAMI      East US       Hub   \n",
            "18   2913     Friday  TURKISH AIRLINES     ISTANBUL  Middle East       Hub   \n",
            "100  2321  Wednesday         SOUTHWEST  LOS ANGELES      West US       Hub   \n",
            "\n",
            "    boarding_area dept_time  wait_min   cleanliness         safety  \\\n",
            "4     Gates 50-59  31-12-18       559  Unacceptable      Very safe   \n",
            "18   Gates 91-102  31-12-18       225  Unacceptable      Very safe   \n",
            "100   Gates 20-39  31-12-18       130  Unacceptable  Somewhat safe   \n",
            "\n",
            "           satisfaction  \n",
            "4    Somewhat satsified  \n",
            "18   Somewhat satsified  \n",
            "100  Somewhat satsified  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categories and Value Constraints**\n",
        "\n",
        "### Capitalize or Lowercase\n",
        "* df['category'] = df['category'].str.upper()\n",
        "* df['category'] = df['category'].str.lower()\n",
        "* df.groupby['category'].count()\n",
        "* df['category'].value_counts()\n",
        "\n",
        "### Leading or Trailing Spaces\n",
        "* df = df['category'].str.strip()\n",
        "\n",
        "### Collapsing Data into Categories\n",
        "* create categories out of income_group column from income column\n",
        "\n",
        "1. using qcut from pandas\n",
        "* group_names = ['0-200K', '200K-500K', '500K+']\n",
        "* demographics = ['income_group'] = pd.qcut(demographics['household_income'], q = 3, labels = group_names)\n",
        "\n",
        "2. using cut from pandas - create category ranges and names\n",
        "* ranges = [0, 200000, 500000, np.inf)\n",
        "* group_names = ['0-200K', '200K-500K', '500K+']\n",
        "* demographics = ['income_group'] = pd.cut(demographics['household_income'], bins = ranges, labels = group_names)\n",
        "\n",
        "\n",
        "###  Collapsing data into categories\n",
        "* map categories to fewer ones\n",
        "\n",
        "1. create mapping dictionary and replace\n",
        "* mapping = {'Microsoft': 'DesktopOS', 'MacOS' : 'DesktopOS', 'IOS' ; 'MobileOS', 'Android' : 'MobileOS'}\n",
        "\n",
        "* devices['operating_system'] = devices['operating_system'].replace(mapping)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eCHaL_ZDDmr1"
      }
    }
  ]
}