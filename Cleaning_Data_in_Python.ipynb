{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKprfFQ91yBfL2yPJ3FBc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirmalaraj77/Python/blob/main/Cleaning_Data_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning Data in Python**\n",
        "\n",
        "* .info()\n",
        "* .describe()\n",
        "* assert\n",
        "* new unnamed index column - index_col = 'Unnamed: 0'\n",
        "* ~ subset\n",
        "* df[['col1', 'col2', 'col5']].sum(axis=1) - sum accross specific rows\n",
        "*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L4EZ2dCUmFjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import practise datasets\n",
        "import pandas as pd\n",
        "!pip install thefuzz\n",
        "\n",
        "airlines = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/airlines_final.csv', index_col = 'Unnamed: 0')\n",
        "banking = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/banking_dirty.csv', index_col = 'Unnamed: 0')\n",
        "ride_sharing = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/ride_sharing_new.csv', index_col = 'Unnamed: 0')\n",
        "restaurants = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/restaurants_L2.csv', index_col = 'Unnamed: 0')\n",
        "restaurants_new = pd.read_csv('https://raw.githubusercontent.com/nirmalaraj77/datasets/refs/heads/main/restaurants_L2_dirty.csv', index_col = 'Unnamed: 0')"
      ],
      "metadata": {
        "id": "oLyqa7dimYCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e582029-ac1a-468c-e1c8-c0e96dcf92db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: thefuzz in /usr/local/lib/python3.10/dist-packages (0.22.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from thefuzz) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **String to Integers**\n",
        "\n",
        "###Remove '£' from Revenue column\n",
        "* Sales['Revenue'] = Sales['Revenue'].str.strip('£')\n",
        "\n",
        "###Convert Revenue column to integer\n",
        "* Sales['Revenue'] = Sales['Revenue'].astype('int')\n",
        "\n",
        "###Verify datatype\n",
        "* assert Sales['Revenue'].dtype == 'int'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g0AB6xEZucsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Numeric to Categorical**\n",
        "\n",
        "###Convert to category\n",
        "* df['col_name'] = df['col_name'].astype('category')\n",
        "\n"
      ],
      "metadata": {
        "id": "h0sDzIT3wNuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())\n",
        "\n",
        "# Convert user_type from integer to category\n",
        "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
        "\n",
        "# Write an assert statement confirming the change\n",
        "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
        "\n",
        "# Print new summary statistics\n",
        "print(ride_sharing['user_type_cat'].describe())"
      ],
      "metadata": {
        "id": "hOzzBodsy_47",
        "outputId": "02fb147b-8375-4f0b-9d20-6f7b889bc75d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 25760 entries, 0 to 25759\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   duration         25760 non-null  object\n",
            " 1   station_A_id     25760 non-null  int64 \n",
            " 2   station_A_name   25760 non-null  object\n",
            " 3   station_B_id     25760 non-null  int64 \n",
            " 4   station_B_name   25760 non-null  object\n",
            " 5   bike_id          25760 non-null  int64 \n",
            " 6   user_type        25760 non-null  int64 \n",
            " 7   user_birth_year  25760 non-null  int64 \n",
            " 8   user_gender      25760 non-null  object\n",
            "dtypes: int64(5), object(4)\n",
            "memory usage: 2.0+ MB\n",
            "None\n",
            "count    25760.000000\n",
            "mean         2.008385\n",
            "std          0.704541\n",
            "min          1.000000\n",
            "25%          2.000000\n",
            "50%          2.000000\n",
            "75%          3.000000\n",
            "max          3.000000\n",
            "Name: user_type, dtype: float64\n",
            "count     25760\n",
            "unique        3\n",
            "top           2\n",
            "freq      12972\n",
            "Name: user_type_cat, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip duration of minutes\n",
        "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
        "\n",
        "# Convert duration to integer\n",
        "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
        "\n",
        "# Write an assert statement making sure of conversion\n",
        "assert ride_sharing['duration_time'].dtype == 'int'\n",
        "\n",
        "# Print formed columns and calculate average ride duration\n",
        "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
        "print(ride_sharing['duration_time'].mean())"
      ],
      "metadata": {
        "id": "Za_LsjdUzCPk",
        "outputId": "1d46cb42-4fb1-471d-c257-37debc2fce5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         duration duration_trim  duration_time\n",
            "0      12 minutes           12              12\n",
            "1      24 minutes           24              24\n",
            "2       8 minutes            8               8\n",
            "3       4 minutes            4               4\n",
            "4      11 minutes           11              11\n",
            "...           ...           ...            ...\n",
            "25755  11 minutes           11              11\n",
            "25756  10 minutes           10              10\n",
            "25757  14 minutes           14              14\n",
            "25758  14 minutes           14              14\n",
            "25759  29 minutes           29              29\n",
            "\n",
            "[25760 rows x 3 columns]\n",
            "11.389052795031056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Out of range dates**\n",
        "\n",
        "* import datetime as dt\n",
        "* today_date = dt.date.today()\n",
        "* find dates in the future\n",
        "* df['subs_date'] > today_date"
      ],
      "metadata": {
        "id": "Z1PAwk2_2LrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "today_date = dt.date.today()\n",
        "today_date\n",
        "birthday = dt.date(1999, 10, 2)\n",
        "today_date < birthday"
      ],
      "metadata": {
        "id": "538ozAVo2wn6",
        "outputId": "e696a7ce-6d35-44ad-d021-9d50792ef606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deal with out of range data**\n",
        "\n",
        "* drop\n",
        "* setting custom minimum and maximums\n",
        "* treat as missing and impute\n",
        "* setting custom value depending on business assumption\n",
        "\n",
        "### Drop values using filtering\n",
        "* movies = movies['rating'] <= 5\n",
        "\n",
        "### Drop values using .drop\n",
        "movies.drop(movies[movies['rating'] > 5].index, inplace = True)\n",
        "\n",
        "### Set custom maximum value\n",
        "* movies.loc[movies['rating'] > 5, 'rating'] = 5\n",
        "\n",
        "### Convert to date\n",
        "* import datetime as dt\n",
        "* movie['signup'] = pd.to_datetime(movie['signup']).dt.date\n",
        "\n",
        "\n",
        "### Drop future dates using filtering or .drop\n",
        "\n",
        "### Hardcode dates with upper limit\n",
        "today_date = dt.date.today()\n",
        "* movies.loc[movies['signup'] > today_date, 'signup'] = today_date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QE_lwxyn4l8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uniqeness Constraints**\n",
        "\n",
        "* **Subsetting on metadata and keeping all duplicate records gives you a better bird-eye's view over your data and how to duplicate it**\n",
        "\n",
        "### Find Duplicates\n",
        "\n",
        "* .duplicated()\n",
        "* True for duplicated and False for non-duplicated\n",
        "* subset: list of column names to check for duplication\n",
        "* keep: whether to keep 'first', 'last' or all ('False') duplicate values\n",
        "* .sort_values(by = 'col_name')\n",
        "\n",
        "### Drop full duplicates\n",
        "\n",
        "* .drop_duplicates()\n",
        "* subset: list of column names to check for duplication\n",
        "* keep: whether to keep 'first', 'last' or all ('False') duplicate values\n",
        "* inplace: Drop duplicated directly in df ('True')\n",
        "\n",
        "### Drop partial duplicates\n",
        "\n",
        "* Group by column names and produce statistical summaries\n",
        "* .goupby() and .agg()\n",
        "* .reset_index() - numbered indices in final output\n",
        "* column_names = ['first_name', 'last_name', 'addres']\n",
        "* summaries = {'height' : 'max', 'weight' : 'mean'}\n",
        "* height_weight = height_weight.groupby(by = column_names).agg(summaries).reset_index()\n",
        "*\n",
        "\n",
        "### E.g.\n",
        "* Find duplicates\n",
        "* duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
        "\n",
        "* Sort your duplicated rides\n",
        "* duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
        "\n",
        "* Print relevant columns\n",
        "* print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
        "\n",
        "### E.g.\n",
        "* Drop complete duplicates from ride_sharing\n",
        "* ride_dup = ride_sharing.drop_duplicates()\n",
        "\n",
        "* Create statistics dictionary for aggregation function\n",
        "* statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
        "\n",
        "* Group by ride_id and compute new statistics\n",
        "* ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
        "\n",
        "* Find duplicated values again\n",
        "* duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
        "* duplicated_rides = ride_unique[duplicates == True]\n",
        "\n",
        "* Assert duplicates are processed\n",
        "* assert duplicated_rides.shape[0] == 0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ulvBBORO4_VA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categories and Membership Constraints**\n",
        "\n",
        "### Find inconsistent categories\n",
        "\n",
        "* .set and .difference\n",
        "* create list of categories\n",
        "* find categories in df not in list\n",
        "* find rows in df matching inconsistent categories\n",
        "* .isin\n",
        "* subset df with boolean result\n",
        "\n",
        "###  Drop inconsistent categories and get consistent categories only\n",
        "\n",
        "* ~ subset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FoB6Vf_x4n03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use airlines dataset\n",
        "airlines.head()\n",
        "\n",
        "# create dictionary of categories\n",
        "cat_all = {'cleanliness' : ['Clean', 'Average', 'Somewhat clean', 'Somewhat dirty', 'Dirty'],\n",
        "           'safety' : ['Neutral', 'Very safe', 'Somewhat safe', 'Very unsafe', 'Somewhat unsafe'],\n",
        "           'satisfaction' : ['Very satisfied', 'Neutral', 'Somewhat satisfied', 'Somewhat unsatisfied', 'Very unsatisfied']}\n",
        "\n",
        "# create categories df\n",
        "categories = pd.DataFrame (cat_all)\n",
        "\n",
        "# Print unique values of survey columns in airlines\n",
        "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
        "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
        "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")\n",
        "\n",
        "# Find the cleanliness category in airlines not in categories\n",
        "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
        "\n",
        "# Find rows with that category\n",
        "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
        "\n",
        "# Print rows with inconsistent category\n",
        "print(airlines[cat_clean_rows])\n",
        "\n",
        "# Print rows with consistent categories only\n",
        "print(airlines[~cat_clean_rows])"
      ],
      "metadata": {
        "id": "coRjPLPH63X0",
        "outputId": "b153c273-97f0-4129-a03f-85c3b5e344bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleanliness:  ['Clean' 'Average' 'Unacceptable' 'Somewhat clean' 'Somewhat dirty'\n",
            " 'Dirty'] \n",
            "\n",
            "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
            "\n",
            "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
            " 'Very unsatisfied'] \n",
            "\n",
            "       id        day           airline  destination  dest_region dest_size  \\\n",
            "4    2992  Wednesday          AMERICAN        MIAMI      east us       Hub   \n",
            "18   2913     Friday  TURKISH AIRLINES     ISTANBUL  middle east       Hub   \n",
            "100  2321  Wednesday         SOUTHWEST  LOS ANGELES      west us       Hub   \n",
            "\n",
            "    boarding_area dept_time  wait_min   cleanliness         safety  \\\n",
            "4     Gates 50-59  31-12-18       559  Unacceptable      Very safe   \n",
            "18   Gates 91-102  31-12-18       225  Unacceptable      Very safe   \n",
            "100   Gates 20-39  31-12-18       130  Unacceptable  Somewhat safe   \n",
            "\n",
            "           satisfaction wait_type day_week  \n",
            "4    Somewhat satsified      long  weekday  \n",
            "18   Somewhat satsified      long  weekday  \n",
            "100  Somewhat satsified    medium  weekday  \n",
            "        id       day        airline        destination    dest_region  \\\n",
            "0     1351   Tuesday    UNITED INTL             KANSAI           asia   \n",
            "1      373    Friday         ALASKA  SAN JOSE DEL CABO  canada/mexico   \n",
            "2     2820  Thursday          DELTA        LOS ANGELES        west us   \n",
            "3     1157   Tuesday      SOUTHWEST        LOS ANGELES        west us   \n",
            "5      634  Thursday         ALASKA             NEWARK        east us   \n",
            "...    ...       ...            ...                ...            ...   \n",
            "2804  1475   Tuesday         ALASKA       NEW YORK-JFK        east us   \n",
            "2805  2222  Thursday      SOUTHWEST            PHOENIX        west us   \n",
            "2806  2684    Friday         UNITED            ORLANDO        east us   \n",
            "2807  2549   Tuesday        JETBLUE         LONG BEACH        west us   \n",
            "2808  2162  Saturday  CHINA EASTERN            QINGDAO           asia   \n",
            "\n",
            "     dest_size boarding_area dept_time  wait_min     cleanliness  \\\n",
            "0          Hub  Gates 91-102  31-12-18       115           Clean   \n",
            "1        Small   Gates 50-59  31-12-18       135           Clean   \n",
            "2          Hub   Gates 40-48  31-12-18        70         Average   \n",
            "3          Hub   Gates 20-39  31-12-18       190           Clean   \n",
            "5          Hub   Gates 50-59  31-12-18       140  Somewhat clean   \n",
            "...        ...           ...       ...       ...             ...   \n",
            "2804       Hub   Gates 50-59  31-12-18       280  Somewhat clean   \n",
            "2805       Hub   Gates 20-39  31-12-18       165           Clean   \n",
            "2806       Hub   Gates 70-90  31-12-18        92           Clean   \n",
            "2807     Small    Gates 1-12  31-12-18        95           Clean   \n",
            "2808     Large    Gates 1-12  31-12-18       220           Clean   \n",
            "\n",
            "             safety        satisfaction wait_type day_week  \n",
            "0           Neutral      Very satisfied    medium  weekday  \n",
            "1         Very safe      Very satisfied    medium  weekday  \n",
            "2     Somewhat safe             Neutral    medium  weekday  \n",
            "3         Very safe  Somewhat satsified      long  weekday  \n",
            "5         Very safe      Very satisfied    medium  weekday  \n",
            "...             ...                 ...       ...      ...  \n",
            "2804        Neutral  Somewhat satsified      long  weekday  \n",
            "2805      Very safe      Very satisfied    medium  weekday  \n",
            "2806      Very safe      Very satisfied    medium  weekday  \n",
            "2807  Somewhat safe      Very satisfied    medium  weekday  \n",
            "2808      Very safe  Somewhat satsified      long  weekend  \n",
            "\n",
            "[2474 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categories and Value Constraints**\n",
        "\n",
        "### Capitalize or Lowercase\n",
        "* df['category'] = df['category'].str.upper()\n",
        "* df['category'] = df['category'].str.lower()\n",
        "* df.groupby['category'].count()\n",
        "* df['category'].value_counts()\n",
        "\n",
        "### Leading or Trailing Spaces\n",
        "* df = df['category'].str.strip()\n",
        "\n",
        "### Collapsing Data into Categories\n",
        "* create categories out of income_group column from income column\n",
        "\n",
        "1. using qcut from pandas\n",
        "* group_names = ['0-200K', '200K-500K', '500K+']\n",
        "* demographics = ['income_group'] = pd.qcut(demographics['household_income'], q = 3, labels = group_names)\n",
        "\n",
        "2. using cut from pandas - create category ranges and names\n",
        "* ranges = [0, 200000, 500000, np.inf)\n",
        "* np.inf - infinity from numpy\n",
        "* group_names = ['0-200K', '200K-500K', '500K+']\n",
        "* demographics = ['income_group'] = pd.cut(demographics['household_income'], bins = ranges, labels = group_names)\n",
        "\n",
        "\n",
        "###  Collapsing data into categories\n",
        "* map categories to fewer ones\n",
        "\n",
        "1. create mapping dictionary and replace\n",
        "* mapping = {'Microsoft': 'DesktopOS', 'MacOS' : 'DesktopOS', 'IOS' ; 'MobileOS', 'Android' : 'MobileOS'}\n",
        "\n",
        "* devices['operating_system'] = devices['operating_system'].replace(mapping)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eCHaL_ZDDmr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n",
        "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
        "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
        "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
        "\n",
        "# Remove white spaces from `dest_size`\n",
        "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
        "\n",
        "# Verify changes have been effected\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra05qfZF3Fvk",
        "outputId": "f2822cc8-f296-4a20-a337-f2f0adbb8cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
            " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
            " 'Australia/New Zealand' 'middle east']\n",
            "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
            " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n",
            "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
            " 'europe' 'central/south america' 'australia/new zealand']\n",
            "['Hub' 'Small' 'Medium' 'Large']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ranges for categories\n",
        "import numpy as np\n",
        "label_ranges = [0, 60, 180, np.inf]\n",
        "label_names = ['short', 'medium', 'long']\n",
        "\n",
        "# Create wait_type column\n",
        "airlines['wait_type'] = pd.cut(airlines['wait_min'],bins = label_ranges,\n",
        "                                labels = label_names)\n",
        "\n",
        "# Create mappings and replace\n",
        "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday',\n",
        "            'Thursday': 'weekday', 'Friday': 'weekday',\n",
        "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
        "\n",
        "airlines['day_week'] = airlines['day'].replace(mappings)\n"
      ],
      "metadata": {
        "id": "ieYmrd-b5-to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Clean Text Data**\n",
        "\n",
        "#### Replace with str.replace\n",
        "* .str.replace('old_str', 'new_str)\n",
        "\n",
        "#### Replace with NumPy NaN using str.len and 'loc\n",
        "* digits = phones['phone_number'].str.len()\n",
        "* phones.loc[digits < 10, 'phone_number'] = np.nan\n",
        "\n",
        "#### Assert (returns nothing if conditon passes)\n",
        "* find length of each row\n",
        "* sanity_check = phone['phone_number'].str.len()\n",
        "* assert min len is 10\n",
        "* assert sanity_check.min() >= 10\n",
        "* assert numbers do not have '+' or '-'\n",
        "* assert phone['phone_numnber'].str.contains(\"+|-\").any() == False\n",
        "\n",
        "#### Clean more complicated examples - Regular Expressions\n",
        "* replace letters with nothing\n",
        "* phones['phone_number'] = phones['phone_number'].str.replace(r'\\D+', '')\n",
        "\n"
      ],
      "metadata": {
        "id": "GkllA2mjAaGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uniformity**\n",
        "\n",
        "* weight in Kg and St, trmperatures in C and F etc..\n",
        "* different date formats\n",
        "\n",
        "\n",
        "#### Temperature data\n",
        "* isolate rows of temperature column above 40\n",
        "* temp_fah = temperartures.loc[temperatures['Temperature'] > 40, 'Temperature']\n",
        "* convert these values to Celcius\n",
        "* temp_cels = (temp_fah - 32) * (5/9)\n",
        "* reassign to respective Farenheit values\n",
        "* temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_cels\n",
        "* assert conversion is correct\n",
        "* assert temperatures['Temperature'].max() < 40\n",
        "\n",
        "#### Datetime formatting\n",
        "--Convert to datetime\n",
        "* birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'],\n",
        "\n",
        "--Attempt to infer format of each date\n",
        "* infer_datetime_format = True,\n",
        "\n",
        "--Return NA for rows where conversion failed (NaT - pandas missing values for datetime onjects)\n",
        "* errors = 'coerce')\n",
        "\n",
        "#### Datetime formatting 2\n",
        "* birthdays['Birthday'] = birthdays['Birthday'].dt.strftime(\"%d-%m-%Y\")\n",
        "\n",
        "#### Is 2019-03-08 in august or March?\n",
        "* convert to NA and treat accordingly\n",
        "* infer format by understanding data source\n",
        "* infer format by understanding previous and sunsequent data in DF\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KbDZocMwjieL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Cross Field Validation**\n",
        "\n",
        "#### Use of multiple fields in a dataset to sanity check data integrity\n",
        "* sum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis=1)\n",
        "* passengers_equ = sum_classes == flights['total_passenger']]\n",
        "* find and filter out rows with inconsistent pannenger totals\n",
        "* inconsistent_pass = flights[~passenger_equ]\n",
        "* consistent_pass = flights[passenger_equ]\n",
        "<br>\n",
        "* convert to datetime and get today date\n",
        "* users['Birthday'] = pd.to_datetime(users['Birthday'])\n",
        "* today = dt.date.today()\n",
        "* for each row in Birthday column, calculate year difference\n",
        "* age_manual = today.year - users['Birthday'].dt.year\n",
        "* find instances where ages match\n",
        "* age_equ = age_manual == users['Age']\n",
        "* find and filter out rows with inconsistent age\n",
        "* inconsistent_age = users[~age_equ]\n",
        "* consistent_age = users[age_equ]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xO79h1gldUGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Missing Data**\n",
        "\n",
        "### Return missing values\n",
        "* df.isna()\n",
        "\n",
        "### Summmary of missingness\n",
        "* df.isna().sum()\n",
        "\n",
        "### Useful packages for visualizing missingness\n",
        "* import missingno as msno\n",
        "* import matplotlib.pyplot as plt\n",
        "* msno.matrix(df)\n",
        "\n",
        "### Airquality example\n",
        "#### isolate missing and complete vales aside\n",
        "* missing = airquality[airquality['CO2'].isna()]\n",
        "* complete = airquality[~airquality['CO2'].isna()]\n",
        "\n",
        "#### Describe complete df\n",
        "* complete.describe()\n",
        "\n",
        "#### Describe incomplete df\n",
        "* incomplwete.describe()\n",
        "\n",
        "#### Visualize with missingno package\n",
        "* sorted_airquality = airquality.sort_values(by = 'Temperature')\n",
        "* msno.matrix(sorted_airquality)\n",
        "\n",
        "### Missingness types\n",
        "#### Missing Completely at Random (MCAR)\n",
        "* no systematic relationship between missing data and other values\n",
        "* data entry errors\n",
        "\n",
        "#### Missing at Random (MAR)\n",
        "* systematic relationship between missing data and other observed values\n",
        "* missing ozone data for high temperature\n",
        "\n",
        "#### Missing Not at Random (MNAR)\n",
        "* systematic relationship between missing data and unobserved values\n",
        "* missing temperature values for high temperatures\n",
        "\n",
        "\n",
        "### Deal with missing data\n",
        "\n",
        "#### Simple approach\n",
        "* Drop missing data\n",
        "* Impute with statistical measures (meran, median, mode)\n",
        "\n",
        "#### More complex approach\n",
        "* Impute using algorithmic approach\n",
        "* Impute with machine learning models\n",
        "\n",
        "### Airquality example 2\n",
        "\n",
        "#### Drop missing values\n",
        "* airquality_dropped = airquality.dropna(subset = ['CO2'])\n",
        "\n",
        "#### Replace with statistical measures\n",
        "* co2_mean = airquality['CO2'].mean()\n",
        "* airquality_imputed = airquality.fillna({'CO2', : co2_mean})\n",
        "\n"
      ],
      "metadata": {
        "id": "y-jE-ygWAS0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comparing Strings**\n",
        "\n",
        "### Minimum edit distance\n",
        "* thefuzz package - Levenshtein algorithm\n",
        "* 0 - 100 score\n",
        "* 0 - not similar at all\n",
        "* 100 - exact match\n",
        "\n",
        "### Partial strings and different orderings\n",
        "* WRatio function is highly robust against partial string comparison with different orderings\n",
        "\n",
        "### Comparision with arrays\n",
        "* We can also compare a string with an array of strings by using the extract function from the process module from fuzzy wuzzy\n",
        "* Extract takes in a string, an array of strings, and the number of possible matches to return ranked from highest to lowest\n",
        "* It returns a list of tuples with 3 elements, the first one being the matching string being returned, the second one being its similarity score, and the third one being its index in the array\n",
        "\n",
        "### Collapsing categories with string similarity\n",
        "* have category df containing correct categories for each state\n",
        "\n",
        "#### for each correct category\n",
        "* for state in categories['state']:\n",
        "#### find potential matches in states with typoes\n",
        "* matches = proces.extract(state, survey['state'], limit = survey.shape[0])\n",
        "#### for each potential match match\n",
        "* for potential_match in matches:\n",
        "#### if high similarity score\n",
        "* if potential_match[1] >= 80:\n",
        "#### replace typo with correct category\n",
        "* survey.loc[survey['state'] == potential_match[0], 'state'] = state\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OiKyBnx2Jl9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare between 2 strings\n",
        "from thefuzz import fuzz\n",
        "fuzz.WRatio('Reeding', 'Reading')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LebLZZL6P6Ah",
        "outputId": "73937e6c-62e4-474d-e1a2-2c1e3a07bdeb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Partial strinng comparision\n",
        "from thefuzz import fuzz\n",
        "fuzz.WRatio('Houston Rockets', 'Rockets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c30c1IYQREcB",
        "outputId": "d591c2cf-6992-42b9-c193-fe965c1c8a12"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare between 2 strings with different ordering\n",
        "from thefuzz import fuzz\n",
        "fuzz.WRatio('Houston Rockets vs Los Angeles Lakers', 'Lakers vs Rockets')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyuJVsfwQeIc",
        "outputId": "890eaf12-ccab-4c0f-fcea-e0dc7d06a2f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparision with arrays #### Comparision with arrays Example\n",
        "\n",
        "# import process\n",
        "from thefuzz import process\n",
        "\n",
        "# define string and array of possible matches\n",
        "string = 'Houston Rockets vs Los Angeles Lakers'\n",
        "choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets', 'Houston vs Los Angeles', 'Heat vs Bulls'])\n",
        "\n",
        "# get matches\n",
        "process.extract(string, choices, limit = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRQyxwq2Q0gR",
        "outputId": "7cf903cf-5d8b-4982-c6a9-dd97920a7eb0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Rockets vs Lakers', 86, 0), ('Lakers vs Rockets', 86, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import process from thefuzz\n",
        "from thefuzz import process\n",
        "\n",
        "# Store the unique values of cuisine_type in unique_types\n",
        "unique_types = restaurants['type'].unique()\n",
        "\n",
        "# Calculate similarity of 'asian' to all values of unique_types\n",
        "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# Calculate similarity of 'american' to all values of unique_types\n",
        "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# Calculate similarity of 'italian' to all values of unique_types\n",
        "print(process.extract('italian', unique_types, limit = len(unique_types)))"
      ],
      "metadata": {
        "id": "BhLTDBh2cR3u",
        "outputId": "64f234b7-349b-4969-f886-841c0634348a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('asian', 100), ('italian', 67), ('american', 62), ('mexican', 50), ('steakhouses', 40), ('cajun', 40), ('southwestern', 36), ('southern', 31), ('coffeebar', 26)]\n",
            "[('american', 100), ('mexican', 80), ('cajun', 68), ('asian', 62), ('italian', 53), ('southwestern', 49), ('southern', 38), ('coffeebar', 24), ('steakhouses', 21)]\n",
            "[('italian', 100), ('asian', 67), ('american', 53), ('mexican', 43), ('cajun', 33), ('southwestern', 33), ('steakhouses', 33), ('southern', 27), ('coffeebar', 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through categories\n",
        "for cuisine in categories:\n",
        "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
        "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "\n",
        "  # Iterate through the list of matches\n",
        "  for match in matches:\n",
        "     # Check whether the similarity score is greater than or equal to 80\n",
        "    if match[1] >= 80:\n",
        "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
        "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
        "\n",
        "# Inspect the final result\n",
        "print(restaurants['cuisine_type'].unique())"
      ],
      "metadata": {
        "id": "4SmKjKRseGOK",
        "outputId": "3db958f2-1eeb-41c7-fc27-578951daee68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'categories' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bd3af2b3caec>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Iterate through categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcuisine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Create a list of matches, comparing cuisine with the cuisine_type column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuisine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestaurants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuisine_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Record Linkge**\n",
        "* Attempts to join data sources that have similarly fuzzy duplicate values\n",
        "* End up with a final DataFrame with no duplicates by using string similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "VWKekU_Jbmvu"
      }
    }
  ]
}